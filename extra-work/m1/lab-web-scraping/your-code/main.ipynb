{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended content.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit the urls below and take a look at their source code through Chrome DevTools. You'll need to identify the html tags, special class names, etc used in the html content you are expected to extract.\n",
    "\n",
    "**Resources**:\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide)\n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are already imported for you. If you prefer to use additional libraries feel free to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here \n",
    "response = requests.get(url)   #Get byte file from url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code   #Print status code of get operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = response.content   # Obtain the content of response\n",
    "#html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Trending  developers on GitHub today · GitHub</title>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_html = BeautifulSoup(html, 'html.parser')   \n",
    "parsed_html.head   \n",
    "parsed_html.body\n",
    "parsed_html.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"px-2 py-4 color-bg-accent-emphasis color-fg-on-emphasis show-on-focus js-skip-to-content\" href=\"#start-of-content\">Skip to content</a>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_html.body.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1 class=\"sr-only\" id=\"search-suggestions-dialog-header\">Search code, repositories, users, issues, pull requests...</h1>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = parsed_html.find('h1')   # Find the elements of article where it has included 'h1'\n",
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain all the fragments where it has been found: 'h1', {'class': 'h3 lh-condensed'}\n",
    "article = parsed_html.find_all('h1', {'class': 'h3 lh-condensed'})   \n",
    "\n",
    "#HELP CODE TO OBTAIN THE SOLUTION\n",
    "#article\n",
    "#article[0].contents\n",
    "#article[0].contents[1].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yudong Jin', 'Vaibhav Srivastav', 'Arseny Kapoulkine', 'Brian Smith', 'Andrew McKnight', 'Laurent Mazare', 'Radamés Ajna', 'Nikita Sobolev', 'lllyasviel', 'Vectorized', 'Stephen Haberman', 'Norman Maurer', 'Stan Girard', 'Andreas Thomas', 'Stella Laurenzo', 'Jake Vanderplas', 'Chris Villa', 'Stephen Celis', \"John O'Reilly\", 'Chris Banes', 'Felix Yan', 'Jinzhe Zeng', 'George Hotz', 'Miguel Ángel Durán', 'Felix Kratz']\n"
     ]
    }
   ],
   "source": [
    "articles = []   #Create empty list\n",
    "\n",
    "# After that, it's possible to obtain all the fragments where it has been found: 'h1', {'class': 'h3 lh-condensed'}\n",
    "for a in article:\n",
    "    a = str(a)   # Transform to string\n",
    "    soup_a = BeautifulSoup(a, 'html.parser')   # Parse each string\n",
    "    list_a = soup_a.find_all('a')   # Append the string of the link to 'articles3' list \n",
    "    for e in list_a:   #With this loop obtain each value from the list\n",
    "        articles.append(e.string.strip())   # With strip, it's possible to obtain the string without the blanks\n",
    "\n",
    "#Solution\n",
    "print(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub.\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url2 = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "response2 = requests.get(url2)   #Get byte file from url\n",
    "response2.status_code   #Print status code of get operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "html2 = response2.content\n",
    "#html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_html2 = BeautifulSoup(html2, 'html.parser')\n",
    "article2 = parsed_html2.find_all('h2', {'class': 'h3 lh-condensed'})\n",
    "\n",
    "#HELP CODE TO OBTAIN THE SOLUTION\n",
    "#article2[0]\n",
    "#len(article2)\n",
    "#article2[0].a.attrs['href']\n",
    "#article2[0].attrs\n",
    "#article2[0].contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AILab-CVC', 'jianchang512', 'Pythagora-io', 'hiroi-sora', 'oobabooga', 'spack', 'safevideo', 'tigerlab-ai', 'kedro-org', 'iterative', 'spdustin', 'dataelement', 'streamlit', 'PromptEngineer48', 'langchain-ai', 'pydantic', 'LorenEteval', 'Kav-K', 'WilliamStar007', 'RekhuGopal', 'shadowsocks', 'hiyouga', 'helblazer811', 'continue-revolution', 'noamgat']\n"
     ]
    }
   ],
   "source": [
    "articles2 = []   #Create empty list\n",
    "# It's possible to obtain all the fragments where it has been found: 'h2', {'class': 'h3 lh-condensed'}\n",
    "for a2 in article2:\n",
    "    a2 = str(a2.a.attrs['href'])   # Extract the link of each photo.\n",
    "    articles2.append(a2.split('/')[1])   # Append the string of the link to 'articles3' list\n",
    "\n",
    "#Solution\n",
    "print(articles2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url3 = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "response3 = requests.get(url3)   #Get byte file from url\n",
    "response3.status_code   #Print status code of get operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "html3 = response3.content   # Obtain the content of response\n",
    "parsed_html3 = BeautifulSoup(html3, 'html.parser')   # Parse the html\n",
    "\n",
    "#In the web of wikipedia there are diferent photos, the first one it's necessary to extract with diferent method to the rest\n",
    "#of the photos.\n",
    "#Code to extract the first photo\n",
    "first_photo = parsed_html3.find_all('span', {'class': 'mw-default-size'})\n",
    "#Code to extract the rest of the photos\n",
    "article3 = parsed_html3.find_all('figure', {'class': 'mw-default-size'})\n",
    "\n",
    "#HELP CODE TO OBTAIN THE SOLUTION\n",
    "#first_photo[0].a\n",
    "#first_photo[0].a.img['src']\n",
    "#first_photo.find('a')['href']\n",
    "#first_photo.get('href')\n",
    "\n",
    "#article3[0].a\n",
    "#article3[0].a.attrs['class']\n",
    "#article3[0].a.img['src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'//upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_photo[0].a.img['src']   # Obtain the link of first photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['//upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG', '//upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Walt_Disney_Birthplace_Exterior_Hermosa_Chicago_Illinois.jpg/220px-Walt_Disney_Birthplace_Exterior_Hermosa_Chicago_Illinois.jpg', '//upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg', '//upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/220px-Steamboat-willie.jpg', '//upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg', '//upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg', '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg', '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg', '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG', '//upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg', '//upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg', '//upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Disney_Oscar_1953_%28cropped%29.jpg/170px-Disney_Oscar_1953_%28cropped%29.jpg']\n"
     ]
    }
   ],
   "source": [
    "articles3 = []   #Create empty list\n",
    "articles3.append(first_photo[0].a.img['src'])   # Append the link of first photo\n",
    "\n",
    "# After that, it's possible to obtain all the fragments where it has been found: 'figure', {'class': 'mw-default-size'}\n",
    "for a3 in article3:\n",
    "    a3 = str(a3.a.img['src'])   # Extract the link of each photo.\n",
    "    articles3.append(a3)   # Append the string of the link to 'articles3' list\n",
    "\n",
    "#Solution\n",
    "print(articles3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url4 ='https://en.wikipedia.org/wiki/Python'\n",
    "response4 = requests.get(url4)   #Get byte file from url\n",
    "response4.status_code   #Print status code of get operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "html4 = response4.content   # Obtain the content of response.\n",
    "parsed_html4 = BeautifulSoup(html4, 'html.parser')   # Parse the html.\n",
    "\n",
    "#In the web of wikipedia there are diferent link of python. Code to extract the links.\n",
    "article4 = parsed_html4.find_all('li')\n",
    "\n",
    "#HELP CODE TO OBTAIN THE SOLUTION\n",
    "#article4 = parsed_html4.find_all('div', {'class': 'mw-parser-output'})\n",
    "#article4[0].a\n",
    "#article4[0].a.attrs['href']\n",
    "#article4[0].a['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://af.wikipedia.org/wiki/Python', 'https://als.wikipedia.org/wiki/Python', 'https://az.wikipedia.org/wiki/Python_(d%C9%99qiql%C9%99%C5%9Fdirm%C9%99)', 'https://be.wikipedia.org/wiki/Python', 'https://cs.wikipedia.org/wiki/Python_(rozcestn%C3%ADk)', 'https://da.wikipedia.org/wiki/Python', 'https://de.wikipedia.org/wiki/Python', 'https://eu.wikipedia.org/wiki/Python_(argipena)', 'https://fr.wikipedia.org/wiki/Python', 'https://hr.wikipedia.org/wiki/Python_(razdvojba)', 'https://id.wikipedia.org/wiki/Python', 'https://ia.wikipedia.org/wiki/Python_(disambiguation)', 'https://is.wikipedia.org/wiki/Python_(a%C3%B0greining)', 'https://it.wikipedia.org/wiki/Python_(disambigua)', 'https://la.wikipedia.org/wiki/Python_(discretiva)', 'https://lb.wikipedia.org/wiki/Python', 'https://hu.wikipedia.org/wiki/Python_(egy%C3%A9rtelm%C5%B1s%C3%ADt%C5%91_lap)', 'https://nl.wikipedia.org/wiki/Python', 'https://pt.wikipedia.org/wiki/Python_(desambigua%C3%A7%C3%A3o)', 'https://ru.wikipedia.org/wiki/Python_(%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D1%8F)', 'https://sk.wikipedia.org/wiki/Python', 'https://sh.wikipedia.org/wiki/Python', 'https://fi.wikipedia.org/wiki/Python', 'https://tr.wikipedia.org/wiki/Python_(anlam_ayr%C4%B1m%C4%B1)', 'https://vi.wikipedia.org/wiki/Python', 'https://zh.wikipedia.org/wiki/Python_(%E6%B6%88%E6%AD%A7%E4%B9%89)', 'https://commons.wikimedia.org/wiki/Category:Python']\n"
     ]
    }
   ],
   "source": [
    "articles4 = []   # Empty list\n",
    "# It's possible to obtain all the fragments where it has been found: 'li'\n",
    "for a4 in article4:\n",
    "    # Check if 'Python' or 'python' are in the tag and if this tag includes 'https:' too.\n",
    "    if ('Python' in str(a4.a) or 'python' in str(a4.a)) and ('https:' in str(a4.a)):\n",
    "        link = str(a4.a['href'])   # Extract 'href' from tag 'a', convert to string and store in link\n",
    "        articles4.append(link)   # Append 'link' to 'articles4'\n",
    "\n",
    "# Solution:\n",
    "print(articles4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the number of titles that have changed in the United States Code since its last release point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url5 = 'http://uscode.house.gov/download/download.shtml'\n",
    "response5 = requests.get(url5)   #Get byte file from url\n",
    "response5.status_code   #Print status code of get operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"usctitlechanged\" id=\"us/usc/t20\">\n",
       "\n",
       "          Title 20 - Education\n",
       "\n",
       "        </div>"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "html5 = response5.content   # Obtain the content of response\n",
    "parsed_html5 = BeautifulSoup(html5, 'html.parser')   # Parse the html\n",
    "\n",
    "article5 = parsed_html5.find_all('div', {'class': 'usctitlechanged'})\n",
    "article5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title 20 - Education', \"Title 38 - Veterans' Benefits ٭\"]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles5 = []   # Create empty list\n",
    "# It's possible to obtain all the fragments where it has been found: ''div', {'class': 'usctitlechanged'}'\n",
    "for a5 in article5:\n",
    "    #Append each text from each element. Delete the blanck from the text and convert to string\n",
    "    articles5.append(str(a5.text.strip()))\n",
    "\n",
    "articles5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find a Python list with the top ten FBI's Most Wanted names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "#url6 = 'https://www.fbi.gov/wanted/topten'\n",
    "url6 = 'https://api.fbi.gov/wanted'\n",
    "response6 = requests.get(url6)   #Get byte file from url\n",
    "response6.status_code   #Print status code of get operation\n",
    "\n",
    "# your code here\n",
    "html6 = response6.content   # Obtain the content of response\n",
    "parsed_html6 = BeautifulSoup(html6, 'html.parser')   # Parse the html\n",
    "#parsed_html6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise - The same data from wikipedia\n",
    "url6 = 'https://en.wikipedia.org/wiki/FBI_Ten_Most_Wanted_Fugitives#:~:text=The%20Criminal%20Investigative%20Division%20(CID,the%20Office%20of%20Public%20Affairs.'\n",
    "response6 = requests.get(url6)   #Get byte file from url\n",
    "response6.status_code   #Print status code of get operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/wiki/Alexis_Flores'"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "html6 = response6.content   # Obtain the content of response\n",
    "parsed_html6 = BeautifulSoup(html6, 'html.parser')   # Parse the html\n",
    "\n",
    "article6 = parsed_html6.find_all('div', {'class': 'center'})\n",
    "article6[0].a['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alexis_Flores',\n",
       " 'Bhadreshkumar_Chetanbhai_Patel',\n",
       " 'Alejandro_Castillo_(criminal)',\n",
       " 'Arnoldo_Jimenez',\n",
       " 'Jose_Rodolfo_Villarreal-Hernandez',\n",
       " 'Yulan_Adonay_Archaga_Carias',\n",
       " 'Ruja_Ignatova',\n",
       " 'Omar_Alexander_Cardenas']"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles6 = []\n",
    "for a6 in article6:\n",
    "    if 'href' in str(a6.a):\n",
    "        string = str(a6.a['href'])\n",
    "        articles6.append(string.split('/')[2])\n",
    "\n",
    "# Solution\n",
    "articles6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Display the 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url7 = 'https://www.emsc-csem.org/Earthquake/'\n",
    "response7 = requests.get(url7)   #Get byte file from url\n",
    "response7.status_code   #Print status code of get operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "html7 = response7.content   # Obtain the content of response\n",
    "parsed_html7 = BeautifulSoup(html7, 'html.parser')   # Parse the html\n",
    "\n",
    "\n",
    "#article7 = parsed_html7.find_all('td', {'class': 'tbdat'})\n",
    "#article7 = article7.find_all(re.compile('lilist^'))\n",
    "#article7[0].a['href']\n",
    "article7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<!DOCTYPE html>\n",
       "\n",
       "<html lang=\"en\"><head><meta charset=\"utf-8\"/><meta content=\"srFzNKBTd0FbRhtnzP--Tjxl01NfbscjYwkp4yOWuQY\" name=\"google-site-verification\"><meta content=\"BCAA3C04C41AE6E6AFAF117B9469C66F\" name=\"msvalidate.01\"/><meta content=\"43b36314ccb77957\" name=\"y_key\"/><meta content=\"all\" name=\"robots\"/><meta content=\"Get informed on the latest earthquakes occurred around the globe. earthquakes today - recent and latest earthquakes, earthquake map and earthquake information. Earthquake information for europe. EMSC (European Mediterranean Seismological Centre) provides real time earthquake information for seismic events with magnitude larger than 5 in the European Mediterranean area and larger than 7 in the rest of the world.\" lang=\"en\" name=\"description\"/><meta content=\"705855916142039\" property=\"fb:app_id\"/><meta content=\"en_FR\" property=\"og:locale\"/><meta content=\"website\" property=\"og:type\"/><meta content=\"EMSC - European-Mediterranean Seismological Centre\" property=\"og:site_name\"/><meta content=\"//www.emsc-csem.org/Earthquake_information/\" property=\"og:url\"/><meta content=\"Earthquake information\" property=\"og:title\"/><meta content=\"Get informed on the latest earthquakes occurred around the globe.\" property=\"og:description\"/><link href=\"/favicon.png\" rel=\"icon\" type=\"image/x-icon\"/>\n",
       "<title>Earthquake information</title>\n",
       "<script> console.log((new Date()).toString());</script><link href=\"//static3.emsc.eu/Css/m_emsc.min.css\" rel=\"stylesheet\"/><script src=\"//static1.emsc.eu/javascript/jquery-3.6.0.min.js\"></script><script> var emsc_ws_url=\"wss://cobra.emsc-csem.org/eq_search\";</script><script src=\"//static1.emsc.eu/javascript/eq_list.min.js?v=1\"></script><script defer=\"\" src=\"//static2.emsc.eu/javascript/eq_list_kml.min.js\"></script><script src=\"//static2.emsc.eu/javascript/emsc.min.js\"></script><style>.eqs{width:100%;table-layout:fixed;border-spacing:0;border-collapse:collapse}.eqs thead,.eqs tbody{width:100%}table.eqs td{padding:8px 3px;font-size:14px}table.eqs th{padding:8px 3px;font-size:16px;background-color:#F8F8F8;border-bottom:2px solid}.tbmag{text-align:center}.tblat,.tblon,.tbdep{text-align:right}.tbreg{width:30%;text-align:left;white-space:nowrap;overflow:hidden;text-overflow:ellipsis}.tbdat{width:20%;text-align:center}.tbdep{padding-right:15px!important}.lilist:hover,.lilist.hover{border:1px solid #eee;font-weight:bold;cursor:pointer}.tago{text-align:left;font-size:10px;color:grey;text-align:center}.evtyp{width:26px;height:26px;background:url(//static1.emsc.eu/Css/img/sprites_eqtype.png);float:right;margin:1px;padding:0;display:inline-block}.bg-type_sonicboom{background-position:-29px -1px}.bg-type_volcano{background-position:-1px -1px}.bg-type_explosion{background-position:-113px -1px}.bg-type_landslide{background-position:-141px -1px}.bg-type_induced{background-position:-57px -1px}.bg-type_tsunami_pending{background-position:-169px -1px}.bg-type_tsunami{background-position:-197px -1px}.bg-type_tsunami_NO{background-position:-85px -1px}.eqs tr:nth-child(2n){background:#ccc}.eqs tr.rw,.eqs tr.bow{font-weight:bold}.eqs tr.rw,tr.rw .tbdat a{color:red}.eqs th>div{font-size:small}.eqs th>span{margin-left:4px;cursor:pointer}.tbmagtyp{display:none}.tbmagtyp.mts{display:table-cell;text-align:right}.sea{text-align:center;margin-bottom:50px}.form,.form form{display:inline-block}.form{margin-top:20px}.form th{padding:5px}.fopen,.form{display:none}.search{cursor:pointer;font-size:16px;font-weight:bold}.fopen,.fclose{margin-right:10px}.rset{margin-left:20px}.with{padding-left:145px}.coords-icon{background:url(//static2.emsc.eu/Css/img/search_map_icon.png);width:106px;height:30px;margin-left:-140px;cursor:pointer;position:absolute}.moref{display:none}.more{display:inline-block;cursor:pointer;border:1px solid;padding:0 4px;border-radius:5px;font-weight:900;font-size:large;vertical-align:top;margin-top:30px}td.ic{text-align:left}#map-coord{position:absolute;left:10%;top:20%;width:80%;height:600px;display:none}.lds-dual-ring{display:inline-block;width:80px;height:80px;position:absolute;left:50%;margin-left:-40px}.lds-dual-ring:after{content:\" \";display:block;width:64px;height:64px;margin:8px;border-radius:50%;border:6px solid #000;border-color:#000 transparent #000 transparent;animation:lds-dual-ring 1.2s linear infinite}@keyframes lds-dual-ring{0%{transform:rotate(0deg)}100%{transform:rotate(360deg)}}.page-cont{text-align:center}.pag{display:inline-block;margin:10px;padding:5px 10px;border:1px solid #eee;background-color:rgb(179,0,18);color:white;cursor:pointer}.selview{background:none;color:rgb(179,0,18);cursor:default}.tbdat a{text-decoration:none;color:black}.citiz{width:80px}.dm{display:inline-block;width:50%;margin-top:3px;text-align:center}.dm.comm:after,.dm.pic:after{width:16px;height:16px;background:url(//static3.emsc.eu/Images/icon/sprite_com_pic.png);display:block;content:\"\";margin:auto}.dm.comm:after{background-position:-19px -1px}.dm.pic:after{background-position:-1px -1px}.links{text-align:center;margin-bottom:1em}.links span{margin:0 5px;cursor:pointer}.cho{color:red;font-weight:bold}.subm{margin-top:15px}.moref label{font-weight:bold;margin-right:10px}#reg{text-transform:uppercase;width:98%}.prop{min-width:200px;position:absolute;z-index:20;height:auto;border:1px solid #666;padding:8px;font-size:10px;font-family:verdana;font-weight:bold;background:white;display:none}.prop .res{width:100%}.prop .res td{cursor:pointer;text-align:left}.prop-close{float:right;color:blue}.prop-send{color:blue;text-decoration:underline;font-weight:bold;cursor:pointer}#nbres{font-weight:bold;color:red;height:20px}.download{text-align:right}.downl{display:none;margin:0 5px}</style></meta></head><body><div class=\"banner\" role=\"banner\"><div class=\"banner-ct\"><div class=\"bann-logo\">\n",
       "<a href=\"/\">\n",
       "<div class=\"spe emsc-logo\"></div>\n",
       "<div class=\"emsc-logo-label\">Centre Sismologique Euro-Méditerranéen</div>\n",
       "<div class=\"emsc-logo-label\">Euro-Mediterranean Seismological Centre</div>\n",
       "</a>\n",
       "</div><div class=\"hmenu\"><div class=\"hmenu0 hmenu1\"><div class=\"hmenus menut mt1\">Earthquakes</div><div class=\"hmenus menus\"><a href=\"//www.emsc-csem.org/Earthquake_map/\">World map</a></div><div class=\"hmenus menus\"><a href=\"//www.emsc-csem.org/Earthquake_information/\">Latest earthquakes</a></div><div class=\"hmenus menus\"><a href=\"//www.emsc-csem.org/Earthquake_data/\">Seismic data</a></div><div class=\"hmenus menus\"><a href=\"//www.emsc-csem.org/Special_reports/\">Special reports</a></div></div><div class=\"hmenu0 hmenu2\"><div class=\"hmenus menut mt2\">LastQuake</div><div class=\"hmenus menus\"><a href=\"//www.emsc-csem.org/lastquake/how_it_works/\">How it works</a></div><div class=\"hmenus menus\"><a href=\"//www.emsc-csem.org/lastquake/information_channels/\">Information channels</a></div><div class=\"hmenus menus\"><a href=\"//www.emsc-csem.org/lastquake/citizen_seismology/\">Citizen seismology</a></div></div><div class=\"hmenu0 hmenu3\"><div class=\"hmenus menut mt3\">About Us</div><div class=\"hmenus menus\"><a href=\"//www.emsc-csem.org/about_us/who_we_are/\">Who we are</a></div><div class=\"hmenus menus\"><a href=\"//www.emsc-csem.org/about_us/what_we_do/\">What we do</a></div><div class=\"hmenus menus\"><a href=\"//www.emsc-csem.org/about_us/timeline/\">Timeline</a></div></div><div class=\"hmenu0 hmenu4\"><div class=\"hmenus menut mt4\">Partner with us</div><div class=\"hmenus menus\"><a href=\"//www.emsc-csem.org/partner_with_us/mission_and_vision/\">Mission &amp; vision</a></div><div class=\"hmenus menus\"><a href=\"//www.emsc-csem.org/partner_with_us/partners/\">Partners</a></div><div class=\"hmenus menus\"><a href=\"//www.emsc-csem.org/partner_with_us/support_our_work/\">Support our work</a></div></div></div><div class=\"btncont\"><a class=\"hbt hbtdonate\" href=\"/donate/\">Donate</a><a class=\"hbt hbtlogin\" href=\"/Member/login.php\">Log in</a></div></div>\n",
       "<div class=\"emsctime\"></div>\n",
       "</div><div class=\"bandeau\"><div class=\"bandeaumv\"></div></div><div class=\"content\" role=\"main\"><div class=\"sea\"><div class=\"search\"><span class=\"fclose\">▶</span><span class=\"fopen\">▼</span>Search earthquakes</div><div class=\"form\">\n",
       "<form>\n",
       "<table><tr><th></th><th>Period</th><th>Magnitude</th><th class=\"moref\">Depth</th><th class=\"moref\">Latitude</th><th class=\"moref\">Longitude</th>\n",
       "<th class=\"moref\"></th><th class=\"moref\">Reports</th><th class=\"moref\">Images</th><th class=\"moref\">Comments</th>\n",
       "</tr>\n",
       "<tr><td>Min: </td><td><input aria-label=\"date min\" id=\"datemin\" max=\"2023-11-05\" min=\"2004-10-01\" name=\"datemin\" type=\"date\"/></td><td><input aria-label=\"magnitude min\" id=\"magmin\" max=\"11\" min=\"0\" name=\"magmin\" step=\"0.1\" type=\"number\"/></td><td><input aria-label=\"depth min\" class=\"moref\" id=\"depthmin\" max=\"10000\" min=\"-10\" name=\"depthmin\" step=\"1\" type=\"number\"/></td>\n",
       "<td><input aria-label=\"latitude min\" class=\"moref\" id=\"latmin\" max=\"90\" min=\"-90\" name=\"latmin\" step=\"0.001\" type=\"number\"/></td>\n",
       "<td class=\"ic\"><input aria-label=\"longitude min\" class=\"moref\" id=\"lonmin\" max=\"180\" min=\"-180\" name=\"lonmin\" step=\"0.001\" type=\"number\"/></td>\n",
       "<td class=\"moref with\"><div class=\"moref coords-icon\" data-maploaded=\"false\"></div>With:</td><td class=\"moref\"><input aria-label=\"with reports\" id=\"reports\" name=\"reports\" type=\"checkbox\"/></td><td class=\"moref\"><input aria-label=\"with medias\" id=\"medias\" name=\"medias\" type=\"checkbox\"/></td><td class=\"moref\"><input aria-label=\"with comments\" id=\"comms\" name=\"comms\" type=\"checkbox\"/></td>\n",
       "</tr>\n",
       "<tr><td>Max: </td><td><input aria-label=\"date max\" id=\"datemax\" max=\"2023-11-05\" min=\"2004-10-01\" name=\"datemax\" type=\"date\"/></td><td><input aria-label=\"magnitude max\" id=\"magmax\" max=\"11\" min=\"0\" name=\"magmax\" step=\"0.1\" type=\"number\"/></td><td><input aria-label=\"depth max\" class=\"moref\" id=\"depthmax\" max=\"10000\" min=\"-10\" name=\"depthmax\" step=\"1\" type=\"number\"/></td>\n",
       "<td><input aria-label=\"latitude max\" class=\"moref\" id=\"latmax\" max=\"90\" min=\"-90\" name=\"latmax\" step=\"0.001\" type=\"number\"/> </td>\n",
       "<td class=\"ic\"><input aria-label=\"longitude max\" class=\"moref\" id=\"lonmax\" max=\"180\" min=\"-180\" name=\"lonmax\" step=\"0.001\" type=\"number\"/></td>\n",
       "<td class=\"moref\"></td><td class=\"moref\"></td><td class=\"moref\"></td><td class=\"moref\"></td>\n",
       "</tr>\n",
       "<tr class=\"moref\"><td><label for=\"flynn_region\">Region Name</label></td><td colspan=\"5\"><input id=\"reg\" name=\"flynn_region\" type=\"text\"/>\n",
       "<div class=\"prop\"><table class=\"res\"><tr><td style=\"width:20px;\"><input class=\"checkall\" type=\"checkbox\"/></td><td><span class=\"prop-close\">[x]</span></td></tr></table>\n",
       "<div class=\"prop-send\">Send checked</div>\n",
       "</div></td></tr>\n",
       "</table>\n",
       "<div class=\"subm\"><input type=\"submit\" value=\"Search\"><input class=\"rset\" type=\"button\" value=\"Reset\"/></input></div>\n",
       "</form>\n",
       "<div class=\"more\">+</div>\n",
       "<div id=\"map-coord\"></div>\n",
       "</div></div><div class=\"links\"><span data-magmin=\"0\">[ Full list ]</span><span data-magmin=\"3\">[ mag≥3 ]</span><span data-magmin=\"4\">[ mag≥4 ]</span><span data-magmin=\"5\">[ mag≥5 ]</span></div><div id=\"nbres\"></div><div class=\"page-cont\"><div class=\"pag spei spei2\">«</div><div class=\"pag spei spei1\">‹</div><div class=\"pag\">1</div><div class=\"pag\">2</div><div class=\"pag\">3</div><div class=\"pag\">4</div><div class=\"pag\">5</div><div class=\"pag\">6</div><div class=\"pag\">7</div><div class=\"pag\">8</div><div class=\"pag\">9</div><div class=\"pag\">10</div><div class=\"pag spes spes1\">›</div><div class=\"pag spes spes2\">»</div></div><div class=\"download\"><a class=\"downl\" href=\"javascript:viewOnMap();\" id=\"view_map\">View on map</a> <a class=\"downl\" href=\"javascript:Emsc_kml._export();\" id=\"download_kml\">Export as KML</a> <a class=\"downl\" href=\"javascript:csv_export();\" id=\"download\">Export as csv</a></div><div class=\"htab\"><table class=\"eqs table-scroll\">\n",
       "<thead><tr><th class=\"thico\"><th class=\"citiz\" colspan=\"2\"><div>Citizen<br/>response</div><div><div class=\"dm comm\"></div><div class=\"dm pic\"></div></div></th>\n",
       "</th><th class=\"tbdat\">Date &amp; Time<div>UTC</div></th><th class=\"tblat\">Lat.<div>degrees</div></th><th class=\"tblon\">Lon.<div>degrees</div></th><th class=\"tbdep\">Depth<div>km</div></th><th class=\"tbmag\">Mag.<span>[+]</span></th><th class=\"tbreg\">Region</th></tr></thead>\n",
       "<tbody></tbody>\n",
       "</table>\n",
       "</div><div class=\"page-cont\"><div class=\"pag spei spei2\">«</div><div class=\"pag spei spei1\">‹</div><div class=\"pag\">1</div><div class=\"pag\">2</div><div class=\"pag\">3</div><div class=\"pag\">4</div><div class=\"pag\">5</div><div class=\"pag\">6</div><div class=\"pag\">7</div><div class=\"pag\">8</div><div class=\"pag\">9</div><div class=\"pag\">10</div><div class=\"pag spes spes1\">›</div><div class=\"pag spes spes2\">»</div></div><div><b>Bold : Earthquakes with a magnitude ≥ 4.5 in Euro-med, or ≥ 5.5 in the world </b><br><b style=\"color:red;\">Red : Earthquakes with a magnitude ≥ 5.0 in Euro-med, or ≥ 6.0 in the world </b><br/></br></div><script>eval(function(p,a,c,k,e,d){e=function(c){return c};if(!''.replace(/^/,String)){while(c--){d[c]=k[c]||c}k=[function(e){return d[e]}];e=function(){return'\\\\w+'};c=1};while(c--){if(k[c]){p=p.replace(new RegExp('\\\\b'+e(c)+'\\\\b','g'),k[c])}}return p}('$(8).1(7(){6.5(\"4 1\");$(\".3\").0();$(\".2\").0()});',9,9,'click|ready|more|search|Doc|log|console|function|document'.split('|'),0,{}))\n",
       "</script><script> console.log(\"TimeLoad\",0.005062103271); </script></div><div class=\"footer\"><div class=\"foot-cont\"><div class=\"part\"><div class=\"foot-logo-label\">EMSC is the European infrastructure for seismological products in</div><a aria-label=\"epos\" href=\"https://www.epos-eu.org/\" target=\"_blank\"><div class=\"spe foot-logo\"></div></a></div><div class=\"part2\"><div class=\"part-middle\"><a href=\"/faq/\">FAQ</a><a class=\"privacy\" href=\"/privacy/index.php\">© 2023 - privacy</a><a href=\"/contact/\">Contact us</a></div></div><div class=\"part p-soc\"><a aria-label=\"facebook EMSC.CSEM\" href=\"https://www.facebook.com/EMSC.CSEM/\" target=\"_blank\"><span class=\"spe f-facebook\"></span></a><a aria-label=\"twitter lastquake\" href=\"https://twitter.com/lastquake\" target=\"_blank\"><span class=\"spe f-twitter\"></span></a><a aria-label=\"linkedin emsc-csem\" href=\"https://www.linkedin.com/company/emsc-csem/\" target=\"_blank\"><span class=\"spe f-linkedin\"></span></a><a aria-label=\"youtube EuroMSC\" href=\"https://www.youtube.com/user/EuroMSC\" target=\"_blank\"><span class=\"spe f-youtube\"></span></a></div></div></div></body></html>"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_html7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the number of tweets by a given Twitter account.\n",
    "Ask the user for the handle (@handle) of a twitter account. You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account\n",
    "Ask the user for the handle (@handle) of a twitter account. You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the top 10 languages by number of native speakers stored in a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display IMDB's top 250 data (movie name, initial release, director name and stars) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = input('Enter the city: ')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the book name, price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (m1_env)",
   "language": "python",
   "name": "m1_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
